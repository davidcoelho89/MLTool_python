{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1>Least-Squares Support Vector machine</h1></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary:\n",
    "\n",
    "1. [Introduction](#introduction)\n",
    "\n",
    "2. [LSSVM CPU Sample](#lssvm_cpu)\n",
    "    \n",
    "3. [LSSVM GPU Sample](#lssvm_gpu)\n",
    "\n",
    "(adapted from https://github.com/RomuloDrumond/LSSVM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction <a class=\"anchor\" id=\"introduction\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Least-Squares Support Vector Machine (LSSVM) is a variation of the original Support Vector Machine (SVM) in which we have a slight change in the objective and restriction functions that results in a big simplification of the optimization problem.\n",
    "\n",
    "First, let's see the optimization problem of an SVM:\n",
    "\n",
    "$$ \n",
    "\\begin{align}\n",
    "    minimize && f_o(\\vec{w},\\vec{\\xi})=\\frac{1}{2} \\vec{w}^T\\vec{w} + C \\sum_{i=1}^{n} \\xi_i &&\\\\\n",
    "    s.t. && d_i(\\vec{w}^T\\vec{x}_i+b)\\geq 1 - \\xi_i, && i = 1,..., n \\\\\n",
    "         && \\xi_i \\geq 0,                            && i = 1,..., n\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "In this case, we have a set of inequality restrictions and when solving the optimization problem by it's dual we find a discriminative function, adding the kernel trick, of the type:\n",
    "\n",
    "\n",
    "$$ f(\\vec{x}) = sign \\ \\Big( \\sum_{i=1}^{n} \\alpha_i^o d_i K(\\vec{x}_i,\\vec{x}) + b_o \\Big) $$\n",
    "\n",
    "Where $\\alpha_i^o$ and $b_o$ denote optimum values. Giving enough regularization (smaller values of $C$) we get a lot of $\\alpha_i^o$ nulls, resulting in a sparse model in which we only need to save the pairs $(\\vec{x}_i,d_i)$ which have the optimum dual variable not null. The vectors $\\vec{x}_i$ with not null $\\alpha_i^o$ are known as support vectors (SV).\n",
    "\n",
    "\n",
    "\n",
    "In the LSSVM case, we change the inequality restrictions to equality restrictions. As the $\\xi_i$ may be negative we square its values in the objective function:\n",
    "\n",
    "$$ \n",
    "\\begin{align}\n",
    "    minimize && f_o(\\vec{w},\\vec{\\xi})=\\frac{1}{2} \\vec{w}^T\\vec{w} + \\gamma \\frac{1}{2}\\sum_{i=1}^{n} \\xi_i^2 &&\\\\\n",
    "    s.t. && d_i(\\vec{w}^T\\vec{x}_i+b) = 1 - \\xi_i, && i = 1,..., n\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "\n",
    "The dual of this optimization problem results in a system of linear equations, a set of Karush-Khun-Tucker (KKT) equations:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix} \n",
    "    0 & \\vec{d}^T \\\\\n",
    "    \\vec{d} & \\Omega + \\gamma^{-1} I \n",
    "\\end{bmatrix}\n",
    "\\\n",
    "\\begin{bmatrix} \n",
    "    b  \\\\\n",
    "    \\vec{\\alpha}\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix} \n",
    "    0 \\\\\n",
    "    \\vec{1}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Where, with the kernel trick, &nbsp; $\\Omega_{i,j} = d_i d_j K(\\vec{x}_i,\\vec{x}_j)$,  &nbsp;  $\\vec{d} = [d_1 \\ d_2 \\ ... \\ d_n]^T$, &nbsp; $\\vec{\\alpha} = [\\alpha_1 \\ \\alpha_2 \\ ... \\ \\alpha_n]^T$ &nbsp;  e &nbsp; $\\vec{1} = [1 \\ 1 \\ ... \\ 1]^T$.\n",
    "\n",
    "The discriminative function of the LSSVM has the same form of the SVM but the $\\alpha_i^o$ aren't usually null, resulting in a bigger model. The big advantage of the LSSVM is in finding it's parameters, which is reduced to solving the linear system of the type:\n",
    "\n",
    "$$ A\\vec{x} = \\vec{b} $$\n",
    "\n",
    "A well-known solution of the linear system is when we minimize the square of the residues, that can be written as the optimization problem:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    minimize && f_o(\\vec{x})=\\frac{1}{2}||A\\vec{x} - \\vec{b}||^2\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "And have the analytical solution:\n",
    "\n",
    "$$ \\vec{x} = A^{\\dagger} \\vec{b} $$\n",
    "\n",
    "Where $A^{\\dagger}$ is the pseudo-inverse defined as:\n",
    "\n",
    "$$ A^{\\dagger} = (A^T A)^{-1} A^T$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. LSSVM CPU Sample <a class=\"anchor\" id=\"lssvm_cpu\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[10.,  0.,  0.],\n",
       "       [ 0.,  8.,  2.],\n",
       "       [ 0.,  1.,  9.]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.9"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Data loading and pre-processing\n",
    "\n",
    "from data_object import *\n",
    "\n",
    "data = DATA()\n",
    "data.class_loading(problem = 'iris')\n",
    "data.label_encode(label_type = 'bipolar')\n",
    "data.hold_out(hold_method = 'aleatory', train_size = 0.8)\n",
    "data.normalize(norm_type = 'zscore')\n",
    "\n",
    "# Classifier Training and Test\n",
    "\n",
    "from lssvm_classifier import *\n",
    "\n",
    "lssvm = LSSVM(gamma=1, kernel='rbf', sigma=1)\n",
    "lssvm.fit(data.X_tr, data.y_tr)\n",
    "y_h = lssvm.predict(data.X_ts)\n",
    "\n",
    "# Classifier Statistics\n",
    "\n",
    "from statistics_object import *\n",
    "\n",
    "stats = STATSCLASS()\n",
    "stats.calculate(data.y_ts,y_h)\n",
    "\n",
    "display(stats.confusion_matrix)\n",
    "display(stats.accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. LSSVM GPU Sample <a class=\"anchor\" id=\"lssvm_gpu\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[11.,  0.,  0.],\n",
       "       [ 0., 10.,  2.],\n",
       "       [ 0.,  0.,  7.]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.9333333333333333"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Data loading and pre-processing\n",
    "\n",
    "from data_object import *\n",
    "\n",
    "data = DATA()\n",
    "data.class_loading(problem = 'iris')\n",
    "data.label_encode(label_type = 'bipolar')\n",
    "data.hold_out(hold_method = 'aleatory', train_size = 0.8)\n",
    "data.normalize(norm_type = 'zscore')\n",
    "\n",
    "# Classifier Training and Test\n",
    "\n",
    "from lssvm_classifier import *\n",
    "\n",
    "lssvm = LSSVM_GPU(gamma=1, kernel='rbf', sigma=1)\n",
    "lssvm.fit(data.X_tr, data.y_tr)\n",
    "y_h = lssvm.predict(data.X_ts)\n",
    "\n",
    "# Classifier Statistics\n",
    "\n",
    "from statistics_object import *\n",
    "\n",
    "stats = STATSCLASS()\n",
    "stats.calculate(data.y_ts,y_h)\n",
    "\n",
    "display(stats.confusion_matrix)\n",
    "display(stats.accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
